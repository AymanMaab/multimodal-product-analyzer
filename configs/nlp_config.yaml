# NLP Model Configuration

model:
  name: "bert-base-uncased"
  type: "bert"  # or "roberta", "distilbert"
  num_classes: 3
  hidden_dim: 256
  dropout: 0.3
  freeze_backbone: false
  max_length: 128

training:
  epochs: 10
  batch_size: 16
  learning_rate: 2.0e-5
  weight_decay: 1.0e-2
  warmup_steps: 500
  gradient_accumulation_steps: 2
  gradient_clip: 1.0

preprocessing:
  lowercase: true
  remove_punctuation: true
  remove_stopwords: false
  lemmatize: true

optimizer:
  type: "adamw"
  betas: [0.9, 0.999]
  eps: 1.0e-8

scheduler:
  type: "linear_warmup"
  num_warmup_steps: 500